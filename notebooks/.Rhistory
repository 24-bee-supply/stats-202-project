model <- lm(formula, data = data)
# Compute LOOCV using the efficient formula
h <- hatvalues(model)
loocv_mse <- mean((residuals(model)/(1 - h))^2)
return(loocv_mse)
}
# Compute LOOCV for each method
loocv_forward <- compute_loocv(selected_vars_forward, df)
loocv_best <- compute_loocv(selected_vars_best, df)
loocv_lasso <- compute_loocv(selected_vars_lasso, df)
# Also fit the models to get R-squared
formula_forward <- as.formula(paste("Ozone ~", paste(selected_vars_forward, collapse = " + ")))
formula_best <- as.formula(paste("Ozone ~", paste(selected_vars_best, collapse = " + ")))
formula_lasso <- as.formula(paste("Ozone ~", paste(selected_vars_lasso, collapse = " + ")))
model_forward <- lm(formula_forward, data = df)
model_best <- lm(formula_best, data = df)
model_lasso <- lm(formula_lasso, data = df)
# Create comparison table
results <- data.frame(
Method = c("Forward Selection", "Best Subsets", "Lasso"),
Num_Variables = c(length(selected_vars_forward),
length(selected_vars_best),
length(selected_vars_lasso)),
Real_Vars = c(sum(!grepl("Fake", selected_vars_forward)),
sum(!grepl("Fake", selected_vars_best)),
sum(!grepl("Fake", selected_vars_lasso))),
Fake_Vars = c(sum(grepl("Fake", selected_vars_forward)),
sum(grepl("Fake", selected_vars_best)),
sum(grepl("Fake", selected_vars_lasso))),
LOOCV_MSE = round(c(loocv_forward, loocv_best, loocv_lasso), 2),
R_squared = round(c(summary(model_forward)$r.squared,
summary(model_best)$r.squared,
summary(model_lasso)$r.squared), 3),
Adj_R_squared = round(c(summary(model_forward)$adj.r.squared,
summary(model_best)$adj.r.squared,
summary(model_lasso)$adj.r.squared), 3)
)
print("Model Comparison:")
print(results)
# Find the best method
best_method_idx <- which.min(results$LOOCV_MSE)
best_method <- results$Method[best_method_idx]
cat("\n")
cat("Best method by LOOCV:", best_method, "\n")
cat("LOOCV MSE:", results$LOOCV_MSE[best_method_idx], "\n")
# Additional analysis for justification
cat("\n--- Additional Analysis ---\n")
cat("Percentage of fake variables selected:\n")
for(i in 1:nrow(results)) {
fake_pct <- round(100 * results$Fake_Vars[i] / results$Num_Variables[i], 1)
cat(sprintf("%s: %.1f%%\n", results$Method[i], fake_pct))
}
# Check variable overlap
common_real <- Reduce(intersect, list(
selected_vars_forward[!grepl("Fake", selected_vars_forward)],
selected_vars_best[!grepl("Fake", selected_vars_best)],
selected_vars_lasso[!grepl("Fake", selected_vars_lasso)]
))
cat("\nReal variables selected by ALL methods:\n")
if(length(common_real) > 0) {
cat(common_real, sep = ", ")
} else {
cat("None")
}
cat("\n")
source("~/Library/Mobile Documents/com~apple~CloudDocs/WBG/CCB/tbw.R", echo=TRUE)
View(all_results)
View(all_results)
View(results_df)
library(shiny); runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v111.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Downloads/v111 (1).R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
runApp('Desktop/WBG/target_dna/v112.R')
library(tidyverse)
library(ISLR)
# Load the data
data(Carseats)
# Set seed for reproducibility
set.seed(42)
# Create train/test split (80/20)
carseats_split <- Carseats %>%
mutate(split = sample(c("train", "test"),
size = n(),
replace = TRUE,
prob = c(0.8, 0.2)))
train <- carseats_split %>%
filter(split == "train") %>%
select(-split)
test <- carseats_split %>%
filter(split == "test") %>%
select(-split)
# Check sizes
cat("Training set size:", nrow(train), "\n")
cat("Test set size:", nrow(test), "\n")
# Fit regression tree
tree_model <- tree(Sales ~ ., data = train)
library(tree)
install.packages("tree")
library(tree)
# Fit regression tree
tree_model <- tree(Sales ~ ., data = train)
# Summary of the tree
summary(tree_model)
plot(tree_model)
text(tree_model, pretty = 0, cex = 0.7, digits = 3)
title("Regression Tree for Carseats Sales")
# Make predictions on test set
predictions <- predict(tree_model, newdata = test)
# Calculate test MSE
test_mse <- mean((test$Sales - predictions)^2)
cat("Test MSE:", test_mse, "\n")
# Optional: Calculate RMSE for interpretation
test_rmse <- sqrt(test_mse)
cat("Test RMSE:", test_rmse, "\n")
library(tree)
library(tidyverse)
# Perform cross-validation to determine optimal tree size
set.seed(42)
cv_tree <- cv.tree(tree_model, FUN = prune.tree)
# Plot cross-validation results
plot(cv_tree$size, cv_tree$dev, type = 'b',
xlab = "Tree Size",
ylab = "Cross-Validation Deviance")
# Find optimal size (minimum CV error)
optimal_size <- cv_tree$size[which.min(cv_tree$dev)]
cat("Optimal tree size:", optimal_size, "\n")
# Prune the tree to optimal size
pruned_tree <- prune.tree(tree_model, best = optimal_size)
# Plot pruned tree
plot(pruned_tree)
text(pruned_tree, pretty = 0, cex = 0.8)
title("Pruned Regression Tree")
# Compare test MSE
# Original tree predictions
orig_pred <- predict(tree_model, newdata = test)
orig_mse <- mean((test$Sales - orig_pred)^2)
# Pruned tree predictions
pruned_pred <- predict(pruned_tree, newdata = test)
pruned_mse <- mean((test$Sales - pruned_pred)^2)
# Display results
cat("\nOriginal tree test MSE:", orig_mse, "\n")
cat("Pruned tree test MSE:", pruned_mse, "\n")
cat("Improvement:", orig_mse - pruned_mse, "\n")
# Summary of both trees
cat("\nOriginal tree size:", sum(tree_model$frame$var == "<leaf>"), "terminal nodes\n")
cat("Pruned tree size:", sum(pruned_tree$frame$var == "<leaf>"), "terminal nodes\n")
library(randomForest)
install.packages("randomForest")
library(randomForest)
library(tidyverse)
# Bagging is just random forest with mtry = p (all predictors)
# For regression, randomForest defaults to mtry = p/3, so we need to specify mtry
set.seed(42)
# Get number of predictors
p <- ncol(train) - 1  # minus 1 for Sales column
# Fit bagging model (mtry = p means use all predictors at each split)
bag_model <- randomForest(Sales ~ .,
data = train,
mtry = p,
importance = TRUE,
ntree = 500)
# Print model
print(bag_model)
# Make predictions on test set
bag_pred <- predict(bag_model, newdata = test)
# Calculate test MSE
bag_mse <- mean((test$Sales - bag_pred)^2)
cat("\nBagging Test MSE:", bag_mse, "\n")
cat("Single Tree Test MSE:", test_mse, "\n")
cat("Improvement:", test_mse - bag_mse, "\n")
# Variable importance
importance(bag_model)
# Plot variable importance
varImpPlot(bag_model, main = "Variable Importance - Bagging")
# Get importance scores in a cleaner format
imp_df <- importance(bag_model) %>%
as.data.frame() %>%
rownames_to_column("Variable") %>%
arrange(desc(`%IncMSE`))
print(imp_df)
library(randomForest)
library(tidyverse)
# Fit random forest with default mtry (p/3 for regression)
set.seed(42)
rf_model <- randomForest(Sales ~ .,
data = train,
importance = TRUE,
ntree = 500)
# Print model
print(rf_model)
# Predictions and test MSE
rf_pred <- predict(rf_model, newdata = test)
rf_mse <- mean((test$Sales - rf_pred)^2)
cat("\nRandom Forest Test MSE:", rf_mse, "\n")
# Variable importance
importance(rf_model)
varImpPlot(rf_model, main = "Variable Importance - Random Forest")
# Compare different mtry values
p <- ncol(train) - 1
mtry_values <- c(1, 2, 3, 4, 5, 7, 9, p)
mse_results <- numeric(length(mtry_values))
set.seed(42)
for(i in 1:length(mtry_values)) {
rf_temp <- randomForest(Sales ~ .,
data = train,
mtry = mtry_values[i],
ntree = 500)
pred_temp <- predict(rf_temp, newdata = test)
mse_results[i] <- mean((test$Sales - pred_temp)^2)
cat("mtry =", mtry_values[i], "Test MSE:", mse_results[i], "\n")
}
# Plot mtry vs MSE
plot(mtry_values, mse_results, type = "b",
xlab = "mtry (number of variables at each split)",
ylab = "Test MSE",
main = "Effect of mtry on Test MSE")
abline(v = p/3, col = "red", lty = 2)
text(p/3, max(mse_results), "default mtry", col = "red", pos = 4)
# Find optimal mtry
optimal_mtry <- mtry_values[which.min(mse_results)]
cat("\nOptimal mtry:", optimal_mtry, "\n")
cat("Best Test MSE:", min(mse_results), "\n")
# Summary comparison
cat("\n--- Summary ---\n")
cat("Single Tree MSE:", test_mse, "\n")
cat("Bagging MSE (mtry = ", p, "):", mse_results[length(mse_results)], "\n")
cat("Random Forest MSE (default mtry):", rf_mse, "\n")
cat("Best Random Forest MSE:", min(mse_results), "\n")
library(BART)
install.packages("BART")
library(BART)
library(tidyverse)
# Prepare data for BART
# BART requires matrices, not data frames
x_train <- model.matrix(Sales ~ ., data = train)[, -1]  # remove intercept
y_train <- train$Sales
x_test <- model.matrix(Sales ~ ., data = test)[, -1]
# Fit BART model
set.seed(42)
bart_model <- gbart(x_train, y_train,
x.test = x_test,
ndpost = 1000,    # number of posterior draws
nskip = 100,      # burn-in
keepevery = 1,    # thinning
printevery = 500) # print progress
# BART provides posterior samples of predictions
# Get posterior mean predictions for test set
bart_pred <- bart_model$yhat.test.mean
# Calculate test MSE
bart_mse <- mean((test$Sales - bart_pred)^2)
cat("\nBART Test MSE:", bart_mse, "\n")
# Plot actual vs predicted
plot(test$Sales, bart_pred,
xlab = "Actual Sales",
ylab = "BART Predicted Sales",
main = "BART: Actual vs Predicted")
abline(0, 1, col = "red")
# Get credible intervals for predictions
# 95% credible intervals
lower <- apply(bart_model$yhat.test, 2, quantile, probs = 0.025)
upper <- apply(bart_model$yhat.test, 2, quantile, probs = 0.975)
# Variable importance in BART
# Count how often each variable is used in splits
var_count <- bart_model$varcount
var_names <- colnames(x_train)
var_importance <- data.frame(
Variable = var_names,
Count = colMeans(var_count),
Proportion = colMeans(var_count) / sum(colMeans(var_count))
) %>%
arrange(desc(Count))
print(var_importance)
# Plot variable importance
barplot(var_importance$Proportion,
names.arg = var_importance$Variable,
las = 2,
main = "BART Variable Importance",
ylab = "Proportion of splits")
# Comparison of all methods
cat("\n--- Final Comparison ---\n")
cat("Single Tree MSE:", test_mse, "\n")
cat("Bagging MSE:", bag_mse, "\n")
cat("Random Forest MSE:", rf_mse, "\n")
cat("BART MSE:", bart_mse, "\n")
# Plot convergence diagnostics
plot(bart_model$sigma, type = "l",
main = "BART: Sigma (error SD) Trace",
xlab = "MCMC Iteration",
ylab = "Sigma")
knitr::opts_chunk$set(echo = TRUE)
#| fig-width: 12
#| fig-height: 20
#| message: false
#| warning: false
library(tidyverse)
library(ggplot2)
library(gridExtra)
train <- read.csv("../data/training.csv")
setwd("~/Documents/stats-202-project/notebooks")
knitr::opts_chunk$set(echo = TRUE)
#| fig-width: 12
#| fig-height: 20
#| message: false
#| warning: false
library(tidyverse)
library(ggplot2)
library(gridExtra)
train <- read.csv("../data/training.csv")
# Get all numeric columns except query_id, url_id, and id
feature_cols <- names(train)
# Create a list to store plots
plots <- list()
# Create simple histograms for each feature
for (col in feature_cols) {
p <- ggplot(train, aes(x = .data[[col]])) +
geom_histogram(bins = 30, fill = "steelblue", color = "black", alpha = 0.7) +
labs(title = paste("Distribution of", col),
x = col,
y = "Count") +
theme_minimal()
plots[[col]] <- p
}
# Display plots in a grid (3 columns)
grid.arrange(grobs = plots, ncol = 3)
#| fig-width: 12
#| fig-height: 20
#| message: false
#| warning: false
# Remove unique identifiers
train_cleaned <- train %>%
select(-id, -query_id)
# Define skewed variables to log-transform (NOT including query_length)
skewed_vars <- c("sig", "sig3", "sig4", "sig5", "sig6")
# Apply log transformation to skewed variables (adding 1 to handle zeros)
train_transformed <- train_cleaned
for (var in skewed_vars) {
if (var %in% names(train_transformed)) {
train_transformed[[paste0("log_", var)]] <- log1p(train_transformed[[var]])
}
}
# Get feature columns (excluding relevance and original skewed variables)
feature_cols <- names(train_transformed)[!names(train_transformed) %in% c("relevance", skewed_vars)]
# Create plots list
plots <- list()
# Create plots for each feature conditioned on relevance
for (col in feature_cols) {
# Convert relevance to factor for better visualization
train_transformed$relevance_factor <- as.factor(train_transformed$relevance)
if (col == "is_homepage") {
# For binary variable, use bar plot
p <- ggplot(train_transformed, aes(x = as.factor(.data[[col]]), fill = relevance_factor)) +
geom_bar(position = "dodge", alpha = 0.7) +
labs(title = paste(col, "by Relevance"),
x = col,
y = "Count",
fill = "Relevance") +
theme_minimal() +
scale_fill_brewer(palette = "Set2")
} else if (col == "query_length") {
# For query_length (count variable), use bar plot
p <- ggplot(train_transformed, aes(x = factor(.data[[col]]), fill = relevance_factor)) +
geom_bar(position = "dodge", alpha = 0.7) +
labs(title = paste(col, "by Relevance"),
x = "Query Length (number of words)",
y = "Count",
fill = "Relevance") +
theme_minimal() +
scale_fill_brewer(palette = "Set2")
} else if (col == "log_sig6") {
# For log_sig6 with many zeros, show zero vs non-zero
train_binary <- train_transformed %>%
mutate(value_category = ifelse(.data[[col]] == 0, "Zero (sig6=0)", "Non-zero (sig6>0)"))
p <- ggplot(train_binary, aes(x = value_category, fill = relevance_factor)) +
geom_bar(position = "dodge", alpha = 0.7) +
labs(title = paste(col, "by Relevance"),
subtitle = "Comparing zero vs non-zero values",
x = "",
y = "Count",
fill = "Relevance") +
theme_minimal() +
scale_fill_brewer(palette = "Set2")
} else {
# Use density plots for other continuous variables
p <- ggplot(train_transformed, aes(x = .data[[col]], fill = relevance_factor)) +
geom_density(alpha = 0.5) +
labs(title = paste(col, "by Relevance"),
x = col,
y = "Density",
fill = "Relevance") +
theme_minimal() +
scale_fill_brewer(palette = "Set2")
}
plots[[col]] <- p
}
# Display plots in grid
grid.arrange(grobs = plots, ncol = 2)
#| fig-width: 12
#| fig-height: 20
#| message: false
#| warning: false
# Remove unique identifiers
train_cleaned <- train %>%
select(-id, -query_id)
# Define skewed variables to log-transform (NOT including query_length)
skewed_vars <- c("sig", "sig3", "sig4", "sig5", "sig6")
# Apply log transformation to skewed variables (adding 1 to handle zeros)
train_transformed <- train_cleaned
for (var in skewed_vars) {
if (var %in% names(train_transformed)) {
train_transformed[[paste0("log_", var)]] <- log1p(train_transformed[[var]])
}
}
# Calculate baseline relevance rate
baseline_relevance <- mean(train_transformed$relevance)
# Get feature columns (excluding relevance and original skewed variables)
feature_cols <- names(train_transformed)[!names(train_transformed) %in% c("relevance", skewed_vars)]
# Create plots list
plots <- list()
# Create plots for each feature conditioned on relevance
for (col in feature_cols) {
# Convert relevance to factor for better visualization
train_transformed$relevance_factor <- as.factor(train_transformed$relevance)
if (col == "is_homepage") {
# For binary variable, use bar plot with proportions
prop_data <- train_transformed %>%
group_by(.data[[col]], relevance_factor) %>%
summarise(count = n()) %>%
group_by(.data[[col]]) %>%
mutate(prop = count / sum(count))
p <- ggplot(prop_data, aes(x = as.factor(.data[[col]]), y = prop, fill = relevance_factor)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
geom_hline(yintercept = baseline_relevance, linetype = "dashed", color = "red", size = 1) +
labs(title = paste(col, "by Relevance"),
x = col,
y = "Proportion",
fill = "Relevance") +
theme_minimal() +
scale_fill_brewer(palette = "Set2")
} else if (col == "query_length") {
# For query_length (count variable), use bar plot with proportions
prop_data <- train_transformed %>%
group_by(.data[[col]], relevance_factor) %>%
summarise(count = n()) %>%
group_by(.data[[col]]) %>%
mutate(prop = count / sum(count))
p <- ggplot(prop_data, aes(x = factor(.data[[col]]), y = prop, fill = relevance_factor)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
geom_hline(yintercept = baseline_relevance, linetype = "dashed", color = "red", size = 1) +
labs(title = paste(col, "by Relevance"),
x = "Query Length (number of words)",
y = "Proportion",
fill = "Relevance") +
theme_minimal() +
scale_fill_brewer(palette = "Set2")
} else if (col == "log_sig6") {
# For log_sig6 with many zeros, show zero vs non-zero with proportions
train_binary <- train_transformed %>%
mutate(value_category = ifelse(.data[[col]] == 0, "Zero (sig6=0)", "Non-zero (sig6>0)"))
prop_data <- train_binary %>%
group_by(value_category, relevance_factor) %>%
summarise(count = n()) %>%
group_by(value_category) %>%
mutate(prop = count / sum(count))
p <- ggplot(prop_data, aes(x = value_category, y = prop, fill = relevance_factor)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
geom_hline(yintercept = baseline_relevance, linetype = "dashed", color = "red", size = 1) +
labs(title = paste(col, "by Relevance"),
subtitle = "Comparing zero vs non-zero values",
x = "",
y = "Proportion",
fill = "Relevance") +
theme_minimal() +
scale_fill_brewer(palette = "Set2")
} else {
# For continuous variables, use smoothed probability curve showing P(relevant|x)
p <- ggplot(train_transformed, aes(x = .data[[col]], y = as.numeric(relevance))) +
geom_smooth(method = "loess", span = 0.3, color = "steelblue", fill = "lightblue", alpha = 0.3) +
geom_hline(yintercept = baseline_relevance, linetype = "dashed", color = "red", size = 1) +
labs(title = paste(col, "by Relevance"),
x = col,
y = "P(Relevant | X)",
subtitle = paste("Red line = baseline (", round(baseline_relevance*100, 1), "%)", sep="")) +
theme_minimal() +
coord_cartesian(ylim = c(0, 1))
}
plots[[col]] <- p
}
# Display plots in grid
grid.arrange(grobs = plots, ncol = 2)
