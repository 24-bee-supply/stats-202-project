---
title: "Stats 202 Analysis"
author: "Your Name"
date: today
format: 
  html:
    toc: true
execute:
  echo: false
  warning: false
  message: false
---

## Selection

I conduct visualizations before preprocessing. Some initial data exploration shows:

- The dependent variable is not imbalanced ($\hat{p} = .4371$)
- `id` and `query_id` are unique identifiers.
- `url_id` is not clearly a unique identifier because the distribution of histogram buckets is not uniform. 
- `is_homepage` is a binary variable. 
- `query_length` is a count.
- `sig`, `sig3`, `sig4`, `sig5`, `sig6` are all highly skewed.

I remove unique identifiers (but not `url_id`, which is ambiguous at this stage) from the training data because they will over-fit the data. 

I visualize the density of each predictor grouped by dependent variable. The overlay of the plots helps visualize whether the variable of interest is systematically higher for certain values of the predictors. To make the contrast more conspicuous, I log-transform the skewed variables. I also group predictor values into deciles and condition on the dependent variable. 

For completness I investigate multicollinearity but find no significant $(|r| > .8)$ correlations. Variance inflation is also more germane to inference than to prediction; the point estimates with multicollinearity do not change even while standard errors increase. 

### Visualizations

#### Distributions of Variables

```{r}
#| fig-width: 12
#| fig-height: 20

#| message: false
#| warning: false

library(tidyverse)
library(ggplot2)
library(gridExtra)
library(corrplot)

# Load data once
train <- read.csv("../data/training.csv")

# Define transformations once
skewed_vars <- c("sig", "sig3", "sig4", "sig5", "sig6")

# Function to prepare data
prepare_data <- function(data) {
  # Remove unique identifiers
  data_cleaned <- data %>%
    select(-id, -query_id)
  
  # Apply log transformation to skewed variables
  data_transformed <- data_cleaned
  for (var in skewed_vars) {
    if (var %in% names(data_transformed)) {
      data_transformed[[paste0("log_", var)]] <- log1p(data_transformed[[var]])
    }
  }
  
  # Convert relevance to factor
  data_transformed$relevance_factor <- as.factor(data_transformed$relevance)
  
  return(data_transformed)
}

# Prepare data once
train_transformed <- prepare_data(train)

# Get feature columns (excluding relevance and original skewed variables)
feature_cols <- names(train_transformed)[!names(train_transformed) %in% c("relevance", "relevance_factor", skewed_vars)]

# Create simple histograms for each feature
plots <- list()

for (col in names(train)) {
  p <- ggplot(train, aes(x = .data[[col]])) +
    geom_histogram(bins = 50, fill = "steelblue", color = "black", alpha = 0.7) +
    labs(title = paste("Distribution of", col),
         x = col,
         y = "Count") +
    theme_minimal()
  
  plots[[col]] <- p
}

# Display plots in a grid (3 columns)
grid.arrange(grobs = plots, ncol = 3)
```

#### Histogram of Select Predictors Conditioned on Dependent Variable

```{r}
#| fig-width: 12
#| fig-height: 20

# Function to create plot based on variable type
create_conditioned_plot <- function(data, col, plot_type = "density") {
  n <- nrow(data)
  
  if (col == "is_homepage") {
    # Binary variable
    p <- ggplot(data, aes(x = as.factor(.data[[col]]), fill = relevance_factor)) +
      geom_bar(aes(y = after_stat(count)/n), position = "dodge", alpha = 0.7) +
      labs(title = paste(col, "by Relevance"),
           x = col,
           y = "Proportion",
           fill = "Relevance")
    
  } else if (col == "query_length") {
    # Count variable
    p <- ggplot(data, aes(x = factor(.data[[col]]), fill = relevance_factor)) +
      geom_bar(aes(y = after_stat(count)/n), position = "dodge", alpha = 0.7) +
      labs(title = paste(col, "by Relevance"),
           x = "Query Length (number of words)",
           y = "Proportion",
           fill = "Relevance")
    
  } else if (plot_type == "density") {
    # Continuous variables - density plot
    p <- ggplot(data, aes(x = .data[[col]], fill = relevance_factor)) +
      geom_density(alpha = 0.5) +
      labs(title = paste(col, "by Relevance"),
           x = col,
           y = "Density",
           fill = "Relevance")
    
  } else {
    # Continuous variables - histogram
    p <- ggplot(data, aes(x = .data[[col]], fill = relevance_factor)) +
      geom_histogram(aes(y = after_stat(count)/n), position = "identity", alpha = 0.5, bins = 30) +
      labs(title = paste(col, "by Relevance"),
           x = col,
           y = "Proportion",
           fill = "Relevance")
  }
  
  # Apply common theme
  p + theme_minimal() + scale_fill_brewer(palette = "Set2")
}

# Create all plots
plots <- lapply(feature_cols, function(col) create_conditioned_plot(train_transformed, col, "density"))

# Display plots in grid
grid.arrange(grobs = plots, ncol = 2)
```

#### Proportions (Bayes) of Select Predictors Conditioned on Dependent Variables Deciles 

```{r}
#| fig-width: 12
#| fig-height: 20

# Function to create binned proportion plots with overall proportion line
create_binned_plot <- function(data, col, n_bins = 10) {
  
  # Calculate overall proportion of relevance = 1
  overall_prop_1 <- mean(data$relevance == 1)
  
  if (col == "is_homepage") {
    # Binary variable
    prop_data <- data %>%
      group_by(.data[[col]], relevance_factor) %>%
      summarise(count = n()) %>%
      group_by(.data[[col]]) %>%
      mutate(prop = count / sum(count))
    
    p <- ggplot(prop_data, aes(x = as.factor(.data[[col]]), y = prop, fill = relevance_factor)) +
      geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
      geom_hline(yintercept = overall_prop_1, linetype = "dashed", color = "red", size = 0.8) +
      labs(title = paste(col, "by Relevance"), x = col)
    
  } else if (col == "query_length") {
    # Count variable
    prop_data <- data %>%
      group_by(.data[[col]], relevance_factor) %>%
      summarise(count = n()) %>%
      group_by(.data[[col]]) %>%
      mutate(prop = count / sum(count))
    
    p <- ggplot(prop_data, aes(x = factor(.data[[col]]), y = prop, fill = relevance_factor)) +
      geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
      geom_hline(yintercept = overall_prop_1, linetype = "dashed", color = "red", size = 0.8) +
      labs(title = paste(col, "by Relevance"), x = "Query Length (number of words)")
    
  } else if (col == "log_sig6") {
    # Special case with many zeros
    data_binary <- data %>%
      mutate(value_category = ifelse(.data[[col]] == 0, "Zero (sig6=0)", "Non-zero (sig6>0)"))
    
    prop_data <- data_binary %>%
      group_by(value_category, relevance_factor) %>%
      summarise(count = n()) %>%
      group_by(value_category) %>%
      mutate(prop = count / sum(count))
    
    p <- ggplot(prop_data, aes(x = value_category, y = prop, fill = relevance_factor)) +
      geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
      geom_hline(yintercept = overall_prop_1, linetype = "dashed", color = "red", size = 0.8) +
      labs(title = paste(col, "by Relevance"),
           subtitle = "Comparing zero vs non-zero values",
           x = "")
    
  } else {
    # Continuous variables - create bins
    data_binned <- data %>%
      mutate(bin = cut(.data[[col]], breaks = n_bins, include.lowest = TRUE))
    
    prop_data <- data_binned %>%
      group_by(bin, relevance_factor) %>%
      summarise(count = n()) %>%
      group_by(bin) %>%
      mutate(prop = count / sum(count)) %>%
      filter(!is.na(bin))
    
    p <- ggplot(prop_data, aes(x = bin, y = prop, fill = relevance_factor)) +
      geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
      geom_hline(yintercept = overall_prop_1, linetype = "dashed", color = "red", size = 0.8) +
      labs(title = paste(col, "by Relevance"), x = col) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
  }
  
  # Apply common styling
  p + 
    labs(y = "Proportion", fill = "Relevance") +
    theme_minimal() +
    scale_fill_brewer(palette = "Set2") +
    annotate("text", x = Inf, y = overall_prop_1, 
             label = paste0("Overall p = ", round(overall_prop_1, 1)), 
             hjust = 1.1, vjust = -0.5, size = 3, color = "red")
}

# Create all plots
plots <- lapply(feature_cols, function(col) create_binned_plot(train_transformed, col))

# Display plots in grid
grid.arrange(grobs = plots, ncol = 2)
```

#### Correlation Plot

```{r}
#| fig-width: 10
#| fig-height: 10
#| message: false
#| warning: false

library(corrplot)

# Select only numeric columns and remove any with issues
numeric_cols <- train_transformed[sapply(train_transformed, is.numeric)]

# Remove the original skewed variables if they exist
cols_to_remove <- c("sig", "sig3", "sig4", "sig5", "sig6")
numeric_cols <- numeric_cols[!names(numeric_cols) %in% cols_to_remove]

# Calculate correlation matrix
cor_matrix <- cor(numeric_cols, use = "complete.obs")

# Create correlation plot
corrplot(cor_matrix, 
         method = "color",
         type = "upper",
         order = "hclust",
         tl.cex = 0.7,
         tl.col = "black",
         tl.srt = 45,
         addCoef.col = "black",
         number.cex = 0.5,
         col = colorRampPalette(c("#FC8D62", "white", "#66C2A5"))(100))
```

## Pre-processing

I code `is_homepage` as an unordered factor. 

I code `query_length` as an unordered factor as well. The decile visualizaiton revealed that the proportion of relevant variables does not increase linearly in `query_length`, and it is therefore appropriate to break the ordinal scale to accomodate unequal distances between each coutn level. 

`url_id` will need to be handled differently. The numeric scale is not sensible but neither are factors, which would lead to over-fitting. Ultimately, it seems risky to include data that may be idiosyncratic, so I exclude it for conservatism. 

`sig2, sig7, and sig8` appear to have regular drop-offs in the histograms. This is an artifact of the measurement scale, which records values in increments of $0.01$. Some histogram bins therefore align with the allowed values and receive higher counts, while others fall between these values and receive few or none, producing the visible pattern of regular dips. This is not a problem for analysis. 

There are no missing values. 

```{r}
## Pre-processing

# Function to preprocess data
preprocess_data <- function(data) {
 
 # Remove unique identifiers
 data_processed <- data %>%
   select(-id, -query_id)
 
 # Remove url_id (idiosyncratic, risk of overfitting)
 data_processed <- data_processed %>%
   select(-url_id)
 
 # Code is_homepage as unordered factor
 data_processed$is_homepage <- as.factor(data_processed$is_homepage)
 
 # Code query_length as unordered factor 
 # (decile visualization revealed non-linear relationship with relevance)
 data_processed$query_length <- as.factor(data_processed$query_length)
 
 return(data_processed)
}

# Apply preprocessing
train_processed <- preprocess_data(train)
```

## Transformation

I apply the following transformations: 

- Log-transform skewed variables (`sig3, sig4, sig5`) for linear models.
- I use `log1p` from base R, which adds a constant of $1$ to each value to avoid issues with zeros.
- The loss of the usual interpretability - where natural logs approximate percentage changes - is not important here.
- Two-part transformation for `sig6`: create a dummy variable indicating zero values, then log-transform the positive values. This preserves the information in zeros (which show a higher proportion of relevance) while making the positive values less skewed.
- Standardize all numeric variables in anticipation of using more flexible, distance-based modeling strategies.

```{r}
# Create a copy of the processed data
train_transformed <- train_processed

# Log-transform skewed variables: sig3, sig4, sig5
train_transformed$log_sig3 <- log1p(train_transformed$sig3)
train_transformed$log_sig4 <- log1p(train_transformed$sig4)
train_transformed$log_sig5 <- log1p(train_transformed$sig5)

# Handle sig6 specially due to many zeros
train_transformed$sig6_is_zero <- as.integer(train_transformed$sig6 == 0)
train_transformed$log_sig6_positive <- ifelse(train_transformed$sig6 > 0, 
                                               log(train_transformed$sig6), 0)

# Remove the original skewed variables
train_transformed <- train_transformed %>%
  select(-sig3, -sig4, -sig5, -sig6)

# Standardize only continuous variables (not factors or binary)
continuous_vars <- c("sig1", "sig2", "sig7", "sig8", 
                    "log_sig3", "log_sig4", "log_sig5", "log_sig6_positive")

for (var in continuous_vars) {
  train_transformed[[var]] <- scale(train_transformed[[var]])[,1]
}

```

## Data Mining

I will use 10-fold cross-validation to estimate generalization error for each modeling strategy. In all cases, I will evaluate model accuracy, picking the tuning parameters and then the strategy which maximizes accuracy. 

My modeling strategies are:

1. LASSO logistic regression: Suitable for correlated predictors and sparse data to reduce variance. Factor variables are one-hot encoded. The tuning variable is $\lambda$, imposing a progressively higher penalty that shrinks coefficients towards zero. 
2. KNN: A simple, intuitive method that makes minimal assumptions about the data’s functional form. Factor variables are one-hot encoded; the curse of dimensionality is not salient given the relatively low dimensions. The tuning variable is $k$, or the number of nearest neighbors. 
3. Random Forests: Well-suited to low-dimensional tabular data and capable of capturing complex, nonlinear interactions automatically without needing the earlier transformations; especially attractive given apparent splines in some variables (`sig1`). Factor variables are not one-hot encoded because one-hot encoding increases the number of predictors, resulting in over-representation of factor variables by random forests. The tuning variable is `mtry`, or the number of predictors randomly sampled from all potential predictors. 

These methods each have the benefit of being relatively interpretable. Many models in the `caret` package automatically pick a variety of tuning parameters. I use this approach except in the case of KNN, where the default $k$ values were too low and therefore needed to be manually adjusted up. 

## Interpretation/Evaluation

KNN performs the best. 

LASSO keeps many `query_length` dummy variables. The coefficients represent changes in the log odds ratio but analysts are discouraged from making inferences about them; it is a known feature of penalized regressions that correlated covariates may be included in the final model without having an underlying causal effect on the dependent variable. 

The random forests model uses a generic 500 trees. `sig2` is the most important feature. When `sig2` is permuted, the model's accuracy drops the most compared to any other predictor, so the forest relies heavily on it for splitting and prediction.

These results generally concur with the earlier visualizations. LASSO emphasizes `query_length13`, which was shown to contain purely relevant observations. `sig2` also appeared to uniquely partition dependent and independent variables. 

Below is a table summarizing results, followed by visualizations of each modeling strategy's variable selection methods and performance. 

```{r}
#| message: false
#| warning: false
library(knitr)
library(kableExtra)
library(tidyverse)

# Collect all model results
model_comparison <- data.frame(
  Model = c("LASSO", "KNN", "Random Forest"),
  CV_Accuracy = c(
    round(1 - min(readRDS("lasso_model.rds")$cv_model$cvm), 4),
    round(max(readRDS("knn_model.rds")$results$Accuracy), 4),
    round(max(readRDS("rf_model.rds")$results$Accuracy), 4)
  ),
  Optimal_Parameter = c(
    paste("λ =", round(readRDS("lasso_model.rds")$cv_model$lambda.min, 4)),
    paste("k =", readRDS("knn_model.rds")$bestTune$k),
    paste("mtry =", readRDS("rf_model.rds")$bestTune$mtry)
  )
)

# Display as formatted table
model_comparison %>%
  kable(col.names = c("Model", "CV Accuracy", "Optimal Parameter"),
        caption = "Model Performance Comparison",
        align = c("l", "c", "c")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE) %>%
  row_spec(which.max(model_comparison$CV_Accuracy), bold = TRUE, color = "white", background = "#4CAF50")
```


### LASSO

```{r}
#| message: false
#| warning: false
library(glmnet)
library(tidyverse)
library(knitr)
library(kableExtra)

# Load model
lasso_results <- readRDS("lasso_model.rds")
cv_lasso <- lasso_results$cv_model
lasso_final <- lasso_results$final_model

# Get coefficients and create table
coef_lasso <- coef(lasso_final)
nonzero_idx <- which(coef_lasso != 0)
nonzero_coefs <- coef_lasso[nonzero_idx]

# Create coefficient table
coef_df <- data.frame(
  Feature = rownames(coef_lasso)[nonzero_idx],
  Coefficient = round(as.vector(nonzero_coefs), 4)
) %>%
  arrange(desc(abs(Coefficient)))

# Display table
coef_df %>%
  kable(caption = paste("LASSO Selected Features (", nrow(coef_df) - 1, " variables + intercept)", sep = ""),
        col.names = c("Feature", "Coefficient"),
        align = c("l", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE) %>%
  row_spec(1, bold = TRUE) %>%  # Bold the intercept
  column_spec(2, color = ifelse(coef_df$Coefficient > 0, "darkgreen", "darkred"))

# Create data frame for ggplot
cv_data <- data.frame(
  log_lambda = log(cv_lasso$lambda),
  cvm = cv_lasso$cvm,
  cvup = cv_lasso$cvup,
  cvlo = cv_lasso$cvlo,
  nzero = cv_lasso$nzero
)

# Create the plot
ggplot(cv_data, aes(x = log_lambda)) +
  geom_ribbon(aes(ymin = cvlo, ymax = cvup), alpha = 0.2, fill = "gray") +
  geom_line(aes(y = cvm), color = "red", size = 1) +
  geom_point(aes(y = cvm), color = "red", size = 2) +
  geom_vline(xintercept = log(cv_lasso$lambda.min), 
             linetype = "dashed", color = "blue", size = 0.8) +
  geom_vline(xintercept = log(cv_lasso$lambda.1se), 
             linetype = "dashed", color = "darkgreen", size = 0.8) +
  
  # Add top axis with fewer labels
  scale_x_continuous(
    name = "Log(λ)",
    sec.axis = sec_axis(~ ., 
                        name = "Number of Features",
                        breaks = log(cv_lasso$lambda)[seq(1, length(cv_lasso$lambda), by = 10)],
                        labels = cv_lasso$nzero[seq(1, length(cv_lasso$lambda), by = 10)])
  ) +
  
  labs(
    title = "LASSO Cross-Validation",
    subtitle = paste("Min λ =", round(cv_lasso$lambda.min, 4), 
                    " | 1SE λ =", round(cv_lasso$lambda.1se, 4)),
    y = "Classification Error",
    caption = paste("Features at min:", cv_lasso$nzero[which.min(cv_lasso$cvm)],
                   "| Features at 1SE:", cv_lasso$nzero[cv_lasso$lambda == cv_lasso$lambda.1se])
  ) +
  
  # Add legend manually with annotation
  annotate("text", x = log(cv_lasso$lambda.min), y = max(cv_data$cvm), 
           label = "λ min", color = "blue", hjust = -0.2, size = 3) +
  annotate("text", x = log(cv_lasso$lambda.1se), y = max(cv_data$cvm), 
           label = "λ 1SE", color = "darkgreen", hjust = -0.2, size = 3) +
  
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5)
  )
```

### KNN

```{r}
library(ggplot2)

# Load model
knn_model <- readRDS("knn_model.rds")

# Plot accuracy vs k
ggplot(knn_model$results, aes(x = k, y = Accuracy)) +
  geom_line(color = "blue", size = 1) +
  geom_point(size = 3, color = "blue") +
  geom_errorbar(aes(ymin = Accuracy - AccuracySD, ymax = Accuracy + AccuracySD), width = 0.5) +
  geom_vline(xintercept = knn_model$bestTune$k, linetype = "dashed", color = "red") +
  labs(title = paste("KNN: Optimal k =", knn_model$bestTune$k),
       x = "k (Number of Neighbors)", 
       y = "10-Fold CV Accuracy") +
  theme_minimal()
```

### Random Forests

```{r}
library(randomForest)
library(ggplot2)
library(caret)

# Load model
rf_model <- readRDS("rf_model.rds")


# Plot accuracy vs mtry
ggplot(rf_model$results, aes(x = mtry, y = Accuracy)) +
  geom_line(color = "darkgreen", size = 1) +
  geom_point(size = 3, color = "darkgreen") +
  geom_errorbar(aes(ymin = Accuracy - AccuracySD, ymax = Accuracy + AccuracySD), width = 0.5) +
  geom_vline(xintercept = rf_model$bestTune$mtry, linetype = "dashed", color = "red") +
  labs(title = paste("Random Forest: Optimal mtry =", rf_model$bestTune$mtry),
       x = "mtry (Features per Split)", 
       y = "10-Fold CV Accuracy") +
  theme_minimal()

# Variable importance
importance_df <- varImp(rf_model)$importance
importance_df$feature <- rownames(importance_df)
importance_df <- importance_df[order(importance_df$Overall, decreasing = TRUE),]

# Plot top 10 important features
top10 <- head(importance_df, 10)
ggplot(top10, aes(x = reorder(feature, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  coord_flip() +
  labs(title = "Top 10 Important Features",
       x = "Feature", y = "Importance") +
  theme_minimal()
```

## Git Repository 
