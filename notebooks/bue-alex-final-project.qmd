---
title: "STATS202 Final Project"
author: "Bue, Alex"
date: today
format: 
 html:
   toc: true
   code-links:
     - text: "GitHub Repository"
       icon: github
       href: https://github.com/24-bee-supply/stats-202-project
execute:
 echo: false
 warning: false
 message: false
---

## Selection

I conduct visualizations first. Some initial data exploration shows:

- The dependent variable is not imbalanced ($\hat{p} = .4371$)
- `id` and `query_id` are unique identifiers.
- `url_id` is not clearly a unique identifier because the distribution of histogram buckets is not uniform. 
- `is_homepage` is a binary variable. 
- `query_length` is a count.
- `sig3`, `sig4`, `sig5`, `sig6` are all highly skewed.

I visualize the density of each predictor grouped by dependent variable. The overlay of the plots helps visualize whether the variable of interest is systematically higher for certain values of the predictors. To make the contrast more conspicuous, I log-transform the skewed variables. I also group predictor values into deciles and condition on the dependent variable. `sig2` appears conspicuously useful as a separator of the dependent variable. 

For completeness I investigate multicollinearity but find no significant $(|r| > .8)$ correlations. Variance inflation is also more germane to inference than to prediction; the point estimates with multicollinearity do not change even while standard errors increase. 

### Visualizations

#### Distributions of Variables

```{r}
#| fig-width: 12
#| fig-height: 20

#| message: false
#| warning: false

library(tidyverse)
library(ggplot2)
library(gridExtra)
library(corrplot)

# Load data once
train <- read.csv("../data/training.csv")

# Define transformations once
skewed_vars <- c("sig", "sig3", "sig4", "sig5", "sig6")

# Function to prepare data
prepare_data <- function(data) {
  # Remove unique identifiers
  data_cleaned <- data %>%
    select(-id, -query_id)
  
  # Apply log transformation to skewed variables
  data_transformed <- data_cleaned
  for (var in skewed_vars) {
    if (var %in% names(data_transformed)) {
      data_transformed[[paste0("log_", var)]] <- log1p(data_transformed[[var]])
    }
  }
  
  # Convert relevance to factor
  data_transformed$relevance_factor <- as.factor(data_transformed$relevance)
  
  return(data_transformed)
}

# Prepare data once
train_transformed <- prepare_data(train)

# Get feature columns (excluding relevance and original skewed variables)
feature_cols <- names(train_transformed)[!names(train_transformed) %in% c("relevance", "relevance_factor", skewed_vars)]

# Create simple histograms for each feature
plots <- list()

for (col in names(train)) {
  p <- ggplot(train, aes(x = .data[[col]])) +
    geom_histogram(bins = 50, fill = "steelblue", color = "black", alpha = 0.7) +
    labs(title = paste("Distribution of", col),
         x = col,
         y = "Count") +
    theme_minimal()
  
  plots[[col]] <- p
}

# Display plots in a grid (3 columns)
grid.arrange(grobs = plots, ncol = 3)
```

#### Histogram of Select Predictors Conditioned on Dependent Variable

```{r}
#| fig-width: 12
#| fig-height: 20

# Function to create plot based on variable type
create_conditioned_plot <- function(data, col, plot_type = "density") {
  n <- nrow(data)
  
  if (col == "is_homepage") {
    # Binary variable
    p <- ggplot(data, aes(x = as.factor(.data[[col]]), fill = relevance_factor)) +
      geom_bar(aes(y = after_stat(count)/n), position = "dodge", alpha = 0.7) +
      labs(title = paste(col, "by Relevance"),
           x = col,
           y = "Proportion",
           fill = "Relevance")
    
  } else if (col == "query_length") {
    # Count variable
    p <- ggplot(data, aes(x = factor(.data[[col]]), fill = relevance_factor)) +
      geom_bar(aes(y = after_stat(count)/n), position = "dodge", alpha = 0.7) +
      labs(title = paste(col, "by Relevance"),
           x = "Query Length (number of words)",
           y = "Proportion",
           fill = "Relevance")
    
  } else if (plot_type == "density") {
    # Continuous variables - density plot
    p <- ggplot(data, aes(x = .data[[col]], fill = relevance_factor)) +
      geom_density(alpha = 0.5) +
      labs(title = paste(col, "by Relevance"),
           x = col,
           y = "Density",
           fill = "Relevance")
    
  } else {
    # Continuous variables - histogram
    p <- ggplot(data, aes(x = .data[[col]], fill = relevance_factor)) +
      geom_histogram(aes(y = after_stat(count)/n), position = "identity", alpha = 0.5, bins = 30) +
      labs(title = paste(col, "by Relevance"),
           x = col,
           y = "Proportion",
           fill = "Relevance")
  }
  
  # Apply common theme
  p + theme_minimal() + scale_fill_brewer(palette = "Set2")
}

# Create all plots
plots <- lapply(feature_cols, function(col) create_conditioned_plot(train_transformed, col, "density"))

# Display plots in grid
grid.arrange(grobs = plots, ncol = 2)
```

#### Proportions of Select Predictors Conditioned on Dependent Variables Deciles 

```{r}
#| fig-width: 12
#| fig-height: 20

# Function to create binned proportion plots with overall proportion line
create_binned_plot <- function(data, col, n_bins = 10) {
  
  # Calculate overall proportion of relevance = 1
  overall_prop_1 <- mean(data$relevance == 1)
  
  if (col == "is_homepage") {
    # Binary variable
    prop_data <- data %>%
      group_by(.data[[col]], relevance_factor) %>%
      summarise(count = n()) %>%
      group_by(.data[[col]]) %>%
      mutate(prop = count / sum(count))
    
    p <- ggplot(prop_data, aes(x = as.factor(.data[[col]]), y = prop, fill = relevance_factor)) +
      geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
      geom_hline(yintercept = overall_prop_1, linetype = "dashed", color = "red", size = 0.8) +
      labs(title = paste(col, "by Relevance"), x = col)
    
  } else if (col == "query_length") {
    # Count variable
    prop_data <- data %>%
      group_by(.data[[col]], relevance_factor) %>%
      summarise(count = n()) %>%
      group_by(.data[[col]]) %>%
      mutate(prop = count / sum(count))
    
    p <- ggplot(prop_data, aes(x = factor(.data[[col]]), y = prop, fill = relevance_factor)) +
      geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
      geom_hline(yintercept = overall_prop_1, linetype = "dashed", color = "red", size = 0.8) +
      labs(title = paste(col, "by Relevance"), x = "Query Length (number of words)")
    
  } else if (col == "log_sig6") {
    # Special case with many zeros
    data_binary <- data %>%
      mutate(value_category = ifelse(.data[[col]] == 0, "Zero (sig6=0)", "Non-zero (sig6>0)"))
    
    prop_data <- data_binary %>%
      group_by(value_category, relevance_factor) %>%
      summarise(count = n()) %>%
      group_by(value_category) %>%
      mutate(prop = count / sum(count))
    
    p <- ggplot(prop_data, aes(x = value_category, y = prop, fill = relevance_factor)) +
      geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
      geom_hline(yintercept = overall_prop_1, linetype = "dashed", color = "red", size = 0.8) +
      labs(title = paste(col, "by Relevance"),
           subtitle = "Comparing zero vs non-zero values",
           x = "")
    
  } else {
    # Continuous variables - create bins
    data_binned <- data %>%
      mutate(bin = cut(.data[[col]], breaks = n_bins, include.lowest = TRUE))
    
    prop_data <- data_binned %>%
      group_by(bin, relevance_factor) %>%
      summarise(count = n()) %>%
      group_by(bin) %>%
      mutate(prop = count / sum(count)) %>%
      filter(!is.na(bin))
    
    p <- ggplot(prop_data, aes(x = bin, y = prop, fill = relevance_factor)) +
      geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
      geom_hline(yintercept = overall_prop_1, linetype = "dashed", color = "red", size = 0.8) +
      labs(title = paste(col, "by Relevance"), x = col) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
  }
  
  # Apply common styling
  p + 
    labs(y = "Proportion", fill = "Relevance") +
    theme_minimal() +
    scale_fill_brewer(palette = "Set2") +
    annotate("text", x = Inf, y = overall_prop_1, 
             label = paste0("Overall p = ", round(overall_prop_1, 1)), 
             hjust = 1.1, vjust = -0.5, size = 3, color = "red")
}

# Create all plots
plots <- lapply(feature_cols, function(col) create_binned_plot(train_transformed, col))

# Display plots in grid
grid.arrange(grobs = plots, ncol = 2)
```

#### Correlation Plot

```{r}
#| fig-width: 10
#| fig-height: 10
#| message: false
#| warning: false

library(corrplot)

# Select only numeric columns and remove any with issues
numeric_cols <- train_transformed[sapply(train_transformed, is.numeric)]

# Remove the original skewed variables if they exist
cols_to_remove <- c("sig", "sig3", "sig4", "sig5", "sig6")
numeric_cols <- numeric_cols[!names(numeric_cols) %in% cols_to_remove]

# Calculate correlation matrix
cor_matrix <- cor(numeric_cols, use = "complete.obs")

# Create correlation plot
corrplot(cor_matrix, 
         method = "color",
         type = "upper",
         order = "hclust",
         tl.cex = 0.7,
         tl.col = "black",
         tl.srt = 45,
         addCoef.col = "black",
         number.cex = 0.5,
         col = colorRampPalette(c("#FC8D62", "white", "#66C2A5"))(100))
```

#### Query Length

Longer query lengths have a very high proportion of relevant entries. I calculate standard errors to see whether these trends are statistically significant or may result in over-fitting. 

```{r}
library(dplyr)
library(ggplot2)

train %>%
  group_by(query_length) %>%
  summarise(
    n = n(),
    prop = mean(relevance == 1),
    .groups = "drop"
  ) %>%
  mutate(
    se = sqrt(prop * (1 - prop) / n),
    lower = prop - 1.96 * se,
    upper = prop + 1.96 * se
  ) %>%
  ggplot(aes(x = as.numeric(as.character(query_length)), y = prop)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.3) +
  geom_hline(yintercept = mean(train_transformed$relevance == 1),
             linetype = "dashed", color = "red") +
  labs(
    x = "Query Length (words)",
    y = "Proportion Relevant",
    title = "Relevance Proportion by Query Length with 95% CI"
  ) +
  theme_minimal()
```

They are not statistically significant. 

### URL ID

```{r}
library(ggplot2)

# Load data
train <- read.csv("../data/training.csv")

# Sort by URL ID and calculate rolling mean
train <- train[order(train$url_id), ]
window <- 1000  # Average over 1000 consecutive URLs
train$rolling_avg <- zoo::rollmean(train$relevance, k = window, fill = NA, align = "center")

# Find where it crosses the overall mean
overall_mean <- mean(train$relevance)
train_clean <- train[!is.na(train$rolling_avg), ]

# Find crossing point
for(i in 2:nrow(train_clean)) {
  if(train_clean$rolling_avg[i-1] > overall_mean & train_clean$rolling_avg[i] <= overall_mean) {
    crossing_point <- train_clean$url_id[i]
    break
  }
}
# Plot
library(viridis)
colors <- viridis(3)

ggplot(train_clean, aes(x = url_id, y = rolling_avg)) +
  geom_line(color = colors[1], size = 1) +
  geom_hline(yintercept = overall_mean, color = colors[2], linetype = "dashed", size = 0.8) +
  geom_vline(xintercept = crossing_point, color = colors[3], size = 1.2) +
  theme_minimal() +
  labs(title = paste("URL ID Relevance - Crosses Mean at URL ID", crossing_point),
       subtitle = "1000-URL Rolling Average",
       x = "URL ID", 
       y = "Average Relevance") +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 11))
```

## Pre-processing

I code `is_homepage` as an unordered factor. 

I remove `query_length` because the most predictive lengths are also the most rare. It is sensible to represent `query_length` as a factor variable, because the proportion of relevance does not appear to increase linearly, but factors introduce various challenges with prediction. If some factor is not present in the future data, then our trained model will not work. A compromise is necessary. 

`url_id` will need to be handled differently. The numeric scale is not sensible but neither are factors, which would lead to over-fitting. Ultimately, it seems risky to include data that may be idiosyncratic, but I retain it because the stakes here are relatively low. 

`sig2, sig7, and sig8` appear to have regular drop-offs in the histograms. This is an artifact of the measurement scale, which records values in increments of $0.01$. Some histogram bins therefore align with the allowed values and receive higher counts, while others fall between these values and receive few or none, producing the visible pattern of regular dips. This is not a problem for analysis. 

There are no missing values. 

```{r}
## Pre-processing

# Load data once
train <- read.csv("../data/training.csv")

# Function to preprocess data
preprocess_data <- function(data) {
 
 # Remove unique identifiers
 data_processed <- data %>%
   select(-id, -query_id)
  
 # Code is_homepage as unordered factor
 data_processed$is_homepage <- as.factor(data_processed$is_homepage)

 return(data_processed)
}

# Apply preprocessing
train_processed <- preprocess_data(train)
```

## Transformation

I apply the following transformations: 

- Log-transform skewed variables (`sig3, sig4, sig5`) for linear models.
  - This linearizes for LASSO, and monotone transformations are unlikely to hurt more flexible modeling strategies. 
  - I use `log1p` from base R, which adds a constant of $1$ to each value to avoid issues with zeros. The loss of the usual interpretability - where natural logs approximate percentage changes - is not important here.
- Two-part transformation for `sig6`: create a dummy variable indicating zero values, then log-transform the positive values. This preserves the information in zeros (which show a higher proportion of relevance) while making the positive values less skewed.
- I add binary variables for `query_length` 1 and 2, which are statistically significant according to my earlier visualization. 
- I add a factor variable for `url_id` below $30000$, the cutoff at which mean relevance was highest. 
- After transforming, I remove `url_id` and other variables. 

Note that I do not standardize at this stage. Since I will later use cross-validation, scaling of the variables should happen within each fold where appropriate. 

```{r}
# Function to apply transformations
apply_transformations <- function(data_processed) {
  # Create a copy of the processed data
  data_transformed <- data_processed
  
  # Log-transform skewed variables: sig3, sig4, sig5
  data_transformed$log_sig3 <- log1p(data_transformed$sig3)
  data_transformed$log_sig4 <- log1p(data_transformed$sig4)
  data_transformed$log_sig5 <- log1p(data_transformed$sig5)

  # Dummy variables for statistically significant word queries
  data_transformed$single_word_query <- as.integer(data_processed$query_length == 1)
  data_transformed$two_word_query <- as.integer(data_processed$query_length == 2)
  data_transformed$three_word_query <- as.integer(data_processed$query_length == 3)
  
  # Handle sig6 specially due to many zeros
  data_transformed$sig6_is_zero <- as.integer(data_transformed$sig6 == 0)
  data_transformed$log_sig6_positive <- ifelse(data_transformed$sig6 > 0, 
                                                log(data_transformed$sig6), 0)
  
  # Create URL ID binary feature (crosses mean at 29879, round to 30000)
  data_transformed$low_url_id <- as.integer(data_transformed$url_id < 30000)
  
  # Remove the original skewed variables and url_id
  data_transformed <- data_transformed %>%
    select(-sig3, -sig4, -sig5, -sig6, -url_id, -query_length)
  
  return(data_transformed)
}

train_transformed <- apply_transformations(train_processed)
```

```{r}
#| message: false
#| warning: false
library(knitr)
library(kableExtra)

# Create a table of transformed variables
transformed_vars <- data.frame(
  Variable = c(
    "relevance",
    "is_homepage",
    "sig1",
    "sig2",
    "log_sig3",
    "log_sig4", 
    "log_sig5",
    "sig6_is_zero",
    "log_sig6_positive",
    "sig7",
    "sig8",
    "single_word_query",
    "two_word_query",
    "three_word_query",
    "low_url_id"
  ),
  Type = c(
    "Binary",
    "Factor",
    "Continuous",
    "Continuous",
    "Continuous",
    "Continuous",
    "Continuous",
    "Binary",
    "Continuous",
    "Continuous",
    "Continuous",
    "Binary",
    "Binary",
    "Binary",
    "Binary"
  ),
  Description = c(
    "Target variable (0 = not relevant, 1 = relevant)",
    "Whether the URL is a homepage",
    "Original signal feature 1",
    "Original signal feature 2",
    "Log-transformed sig3",
    "Log-transformed sig4",
    "Log-transformed sig5",
    "Indicator for sig6 = 0",
    "Log of sig6 when positive",
    "Original signal feature 7",
    "Original signal feature 8",
    "Query length = 1",
    "Query length = 2",
    "Query length = 3",
    "URL ID < 30000"
  ),
  Transformation = c(
    "None",
    "Factor",
    "None",
    "None",
    "log1p(sig3)",
    "log1p(sig4)",
    "log1p(sig5)",
    "sig6 == 0",
    "log(sig6) if sig6 > 0",
    "None",
    "None",
    "query_length == 1",
    "query_length == 2",
    "query_length == 3",
    "url_id < 30000"
  )
)

# Display the table
transformed_vars %>%
  kable(
    caption = "Summary of Transformed Variables for Modeling",
    col.names = c("Variable", "Type", "Description", "Transformation"),
    align = c("l", "c", "l", "l")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = TRUE
  )
```

## Data Mining

I will use 10-fold cross-validation to estimate generalization error for each modeling strategy. In all cases, I will evaluate model accuracy, picking the tuning parameters and then the strategy which maximizes accuracy. 

My modeling strategies are:

1. LASSO logistic regression: Suitable for correlated predictors and sparse data to reduce variance. Factor variables are one-hot encoded. The tuning variable is $\lambda$, imposing a progressively higher penalty that shrinks coefficients towards zero. I use a "kitchen-sink" approach with all interactions.
2. KNN: A simple, intuitive method that makes minimal assumptions about the data’s functional form. Factor variables are one-hot encoded; the curse of dimensionality is not salient given the relatively low dimensions. The tuning variable is $k$, or the number of nearest neighbors. 
3. Random Forests: Well-suited to low-dimensional tabular data and capable of capturing complex, nonlinear interactions automatically without needing the earlier transformations; especially attractive given apparent splines in some variables (`sig1`). Factor variables are not one-hot encoded because one-hot encoding increases the number of predictors, resulting in over-representation of factor variables by random forests. The tuning parameters are below:

```r
rf_grid <- expand.grid(
  mtry = c(2, 4, 6, default_mtry),
  splitrule = "gini",
  min.node.size = c(1, 5, 10)
)
```

4. I also use boosting with XGBoost. The tuning parameters are below:

```r
params <- list(
  objective = "binary:logistic",
  eval_metric = "error",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)
```

Computational constraints limit somewhat my range of tuning parameters. 

## Interpretation/Evaluation

Boosting with XGBoost performs the best.

LASSO keeps many variables. The coefficients represent changes in the log odds ratio but analysts are discouraged from making inferences about them; it is a known feature of penalized regressions that correlated covariates may be included in the final model without having an underlying causal effect on the dependent variable. 

The random forests model uses a generic 500 trees. `sig2` is the most important feature. When `sig2` is permuted, the model's accuracy drops the most compared to any other predictor, so the forest relies heavily on it for splitting and prediction.

These results generally concur with the earlier visualizations. `sig2` appeared to uniquely partition dependent and independent variables. 

Below is a table summarizing results, followed by visualizations of each modeling strategy's variable selection methods and performance. 

```{r}
#| message: false
#| warning: false
library(knitr)
library(kableExtra)
library(tidyverse)
# Collect all model results
model_comparison <- data.frame(
  Model = c("LASSO", "KNN", "Random Forest", "XGBoost"),
  CV_Accuracy = c(
    round(1 - min(readRDS("../models/lasso_model.rds")$cv_model$cvm), 5),
    round(max(readRDS("../models/knn_model.rds")$results$Accuracy), 5),
    round(max(readRDS("../models/rf_ranger_model.rds")$results$Accuracy), 5),
    round(1 - readRDS("../models/xgb_model.rds")$cv$evaluation_log$test_error_mean[readRDS("../models/xgb_model.rds")$cv$best_iteration], 5)
  ),
  Optimal_Parameter = c(
    paste("λ =", round(readRDS("../models/lasso_model.rds")$cv_model$lambda.min, 4)),
    paste("k =", readRDS("../models/knn_model.rds")$bestTune$k),
    paste("mtry =", readRDS("../models/rf_ranger_model.rds")$bestTune$mtry),
    paste("iterations =", readRDS("../models/xgb_model.rds")$cv$best_iteration)
  )
)

# Display as formatted table
model_comparison %>%
  kable(col.names = c("Model", "CV Accuracy", "Optimal Parameters"),
        caption = "Model Performance Comparison",
        align = c("l", "c", "l")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE) %>%
  row_spec(which.max(model_comparison$CV_Accuracy), bold = TRUE, color = "white", background = "#4CAF50")
```


### LASSO

```{r}
#| message: false
#| warning: false
library(glmnet)
library(tidyverse)
library(knitr)
library(kableExtra)

# Load model
lasso_results <- readRDS("../models/lasso_model.rds")
cv_lasso <- lasso_results$cv_model
lasso_final <- lasso_results$final_model

# Get coefficients and create table
coef_lasso <- coef(lasso_final)
nonzero_idx <- which(coef_lasso != 0)
nonzero_coefs <- coef_lasso[nonzero_idx]

# Create coefficient table
coef_df <- data.frame(
  Feature = rownames(coef_lasso)[nonzero_idx],
  Coefficient = round(as.vector(nonzero_coefs), 4)
) %>%
  arrange(desc(abs(Coefficient)))

# Display table
coef_df %>% 
  head() %>%
  kable(caption = paste("Head of LASSO Selected Features (", nrow(coef_df) - 1, " variables + intercept)", sep = ""),
        col.names = c("Feature", "Coefficient"),
        align = c("l", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE) %>%
  row_spec(1, bold = TRUE) %>%  # Bold the intercept
  column_spec(2, color = ifelse(coef_df$Coefficient > 0, "darkgreen", "darkred")) 

# Create data frame for ggplot
cv_data <- data.frame(
  log_lambda = log(cv_lasso$lambda),
  cvm = cv_lasso$cvm,
  cvup = cv_lasso$cvup,
  cvlo = cv_lasso$cvlo,
  nzero = cv_lasso$nzero
)

# Create the plot
ggplot(cv_data, aes(x = log_lambda)) +
  geom_ribbon(aes(ymin = cvlo, ymax = cvup), alpha = 0.2, fill = "gray") +
  geom_line(aes(y = cvm), color = "red", size = 1) +
  geom_point(aes(y = cvm), color = "red", size = 2) +
  geom_vline(xintercept = log(cv_lasso$lambda.min), 
             linetype = "dashed", color = "blue", size = 0.8) +
  geom_vline(xintercept = log(cv_lasso$lambda.1se), 
             linetype = "dashed", color = "darkgreen", size = 0.8) +
  
  # Add top axis with fewer labels
  scale_x_continuous(
    name = "Log(λ)",
    sec.axis = sec_axis(~ ., 
                        name = "Number of Features",
                        breaks = log(cv_lasso$lambda)[seq(1, length(cv_lasso$lambda), by = 10)],
                        labels = cv_lasso$nzero[seq(1, length(cv_lasso$lambda), by = 10)])
  ) +
  
  labs(
    title = "LASSO Cross-Validation",
    subtitle = paste("Min λ =", round(cv_lasso$lambda.min, 4), 
                    " | 1SE λ =", round(cv_lasso$lambda.1se, 4)),
    y = "Classification Error",
    caption = paste("Features at min:", cv_lasso$nzero[which.min(cv_lasso$cvm)],
                   "| Features at 1SE:", cv_lasso$nzero[cv_lasso$lambda == cv_lasso$lambda.1se])
  ) +
  
  # Add legend manually with annotation
  annotate("text", x = log(cv_lasso$lambda.min), y = max(cv_data$cvm), 
           label = "λ min", color = "blue", hjust = -0.2, size = 3) +
  annotate("text", x = log(cv_lasso$lambda.1se), y = max(cv_data$cvm), 
           label = "λ 1SE", color = "darkgreen", hjust = -0.2, size = 3) +
  
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5)
  )
```

### KNN

```{r}
library(ggplot2)

# Load model
knn_model <- readRDS("../models/knn_model.rds")

# Plot accuracy vs k
ggplot(knn_model$results, aes(x = k, y = Accuracy)) +
  geom_line(color = "blue", size = 1) +
  geom_point(size = 3, color = "blue") +
  geom_errorbar(aes(ymin = Accuracy - AccuracySD, ymax = Accuracy + AccuracySD), width = 0.5) +
  geom_vline(xintercept = knn_model$bestTune$k, linetype = "dashed", color = "red") +
  labs(title = paste("KNN: Optimal k =", knn_model$bestTune$k),
       x = "k (Number of Neighbors)", 
       y = "10-Fold CV Accuracy") +
  theme_minimal()
```

### Random Forests

```{r}
library(randomForest)
library(ggplot2)
library(caret)
library(dplyr)

# Load model
rf_model <- readRDS("../models/rf_ranger_model.rds")

# Plot accuracy vs mtry
ggplot(rf_model$results, aes(x = mtry, y = Accuracy)) +
  geom_line(color = "darkgreen", size = 1) +
  geom_point(size = 3, color = "darkgreen") +
  geom_errorbar(aes(ymin = Accuracy - AccuracySD, ymax = Accuracy + AccuracySD), width = 0.5) +
  geom_vline(xintercept = rf_model$bestTune$mtry, linetype = "dashed", color = "red") +
  labs(title = paste("Random Forest: Optimal mtry =", rf_model$bestTune$mtry),
       x = "mtry (Features per Split)", 
       y = "10-Fold CV Accuracy") +
  theme_minimal()

# Variable importance - SIMPLIFIED
importance_obj <- varImp(rf_model, scale = FALSE)
importance_df <- as.data.frame(importance_obj$importance)
importance_df$feature <- rownames(importance_df)

# Get the first column of importance (whatever it's named)
importance_df$importance_value <- importance_df[,1]

# Get top 10 and plot
top10 <- head(importance_df, 10)

ggplot(top10, aes(x = reorder(feature, importance_value), y = importance_value)) +
 geom_bar(stat = "identity", fill = "darkgreen") +
 coord_flip() +
 labs(title = "Top 10 Important Features",
      x = "Feature", y = "Importance") +
 theme_minimal()
```

### XGBoost

```{r}
library(xgboost)
library(ggplot2)
library(dplyr)

# Load model
xgb_results <- readRDS("../models/xgb_model.rds")
xgb_model <- xgb_results$model
cv_results <- xgb_results$cv

# Plot CV results
cv_df <- cv_results$evaluation_log
ggplot(cv_df, aes(x = iter, y = test_error_mean)) +
  geom_line(color = "darkblue", size = 1) +
  geom_ribbon(aes(ymin = test_error_mean - test_error_std, 
                  ymax = test_error_mean + test_error_std), 
              alpha = 0.2, fill = "darkblue") +
  geom_vline(xintercept = cv_results$best_iteration, linetype = "dashed", color = "red") +
  labs(title = paste("XGBoost: Optimal iteration =", cv_results$best_iteration),
       x = "Iteration", 
       y = "Test Error (10-Fold CV)") +
  theme_minimal()

# Feature importance
importance_matrix <- xgb.importance(model = xgb_model)

# Plot top 10 features
importance_matrix %>%
  head(10) %>%
  ggplot(aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "darkblue") +
  coord_flip() +
  labs(title = "XGBoost: Top 10 Important Features",
       x = "Feature", y = "Importance (Gain)") +
  theme_minimal()

# Print accuracy
best_accuracy <- round(1 - cv_df$test_error_mean[cv_results$best_iteration], 4)
```

## Testing

A script for creating the test submissions is included in the GitHub Repository: https://github.com/24-bee-supply/stats-202-project.

**## Conclusion**

This was an instructive project. Textbooks, forums, and large language models have lowered the barriers to using the sophisticated data-mining techniques from this class. Most students’ results were relatively similar, but the top performers tended to share two strategies:

1. They engineered new features.
2. They iterated on their initial best-performing model.

My approach was more modest. I prioritized visualizing the data to better understand its functional form. I had assumed this emphasis would be relatively unique given the temptation to simply train a large, powerful neural network. But my efforts were incomplete. The log-transformations I applied, as noted earlier, theoretically mattered for linear models but not for more flexible ones. My attempts to find “signal” in the noise through data exploration should have been more analytical and model-agnostic. Including `url_id` was also almost certainly a mistake. 

This experience highlights areas where I am eager to grow as a data scientist, particularly in embeddings, careful attention to the test data distribution, and quantifying over-fitting.

Dr. Tranh helped solidify a critical but, until recently, under-appreciated concept for me: it is important to make learning as easy as possible for the model. Powerful, flexible modeling strategies do not remove the need to understand and thoughtfully shape your data so that the problem is better-conditioned. Statistically, sprawling and complex models may achieve low bias but often at the cost of high variance, a pattern evident in the Kaggle competition’s private scores. Every machine learning course covers the bias-variance decomposition, but recognizing why simplicity matters in practice is, I think, a hallmark of mature modeling. Of course, it is also possible that I am wrong and complexity often pays. This is an area I am excited to explore.

Dr. Tranh’s emphasis on understanding your test data, even gaming the system if possible, was another important lesson. At various points in my analysis, especially involving the `url_id` and `query_length` variables, it occurred to me that certain assumptions might not generalize. Dogmatically, I refused to model the test data, but in real-world applications it is important to investigate whether your training data actually resembles the test data.

Finally, I am eager to learn more about learning curves and other diagnostics of over-fitting. It is easy to run cross-validation, optimize a loss function, and conclude that a model is adequate. I want to go further—developing analyses that quantify over-fitting and pinpoint how and where a model fails compared to alternatives. For example, the team that won the first stage of the competition but lost the second likely relied on cross-validation to estimate generalization error, yet their model underperformed against one that remained consistent across both stages. I want to understand and apply diagnostics that can detect such issues, even when the modeling process appears rigorous.
